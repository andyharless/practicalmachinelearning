---
title: "Pracical Machine Learning Course Project"
author: "Andy Harless"
date: "March 14, 2017"
output: 
  html_document: 
    keep_md: yes
---

# Human Activity Recognition: Weight Lifting


## Backgound

As described in [this paper](http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf), Velloso, Bulling, Gellersen, Ugulino, and Fuks built a data set of body sensor data for weight lifting exercises performed in various ways.  Lifters were asked to perform the exercise either correctly (label A) or with one of four common incorrect variations (labels B, C, D, and E).  The objective of this study is to predict the labels based on the sensor data in the data set.


## Initial Data Processing

Many of the columns in the data set refer to summary statistics which are available only once for each data collection window.  These statistics are also not present in the test cases provided for this course and thus would not be useful for this study.  Accordingly I deleted those columns from the training set.  Once those columns were deleted, there were only three pieces of data missing, and I deleted the cases that contained them.  The resulting set of training data had 19621 labelled observations of 52 potential predictor variables.  Lacking any particular substantive knowledge of the material, I decided, at least as a first cut, to use all 52 predictors without any feature-specific preprocessing.  For fitting purposes, I preprocessed all variables using the Box-Cox procedure, as offered via the "caret" R package.

```{r}
library(caret)
```

## Modeling and Cross-Validation Strategy

I modeled the data using a variety of approaches available in the "caret" R package.  I divided the data into three subsets:  an initial training set (60%), an initial validation set (20%), and a preliminary test set (20%).  The overall strategy was as follows:

1. Use initial training set to fit alternative models.

2. Compare models' performance on the validation set.

3. Combine the best performing models using a "weighted majority vote" strategy.

4. Re-fit the models on an augmented training set that includes the validation data.

5. Use the preliminary test set to estimate performance and to make a final model choice if not resolved in the previous step.

6. Re-fit the chosen model(s) to the entire data set to generate parameters for prediction on the unlabeled test cases.

It should be noted that many of the models available in the "caret" package do their own cross-validation during the fitting process, so there are several levels of cross-validation involved in the overall strategy described here.

## Choice of Models

I chose models according to the following criteria:

- Represent a broad variety of approaches.

- Canned version of model available in the "caret" package will run on this data set on my computer without major errors.

- Canned version trains in less than an hour on this data set on my computer (using 3 of the i7's 4 cores in parallel).

- Stuff that looked interesting.

- Random trial and error.

Here is the final list:

- Linear Discriminant Analysis ("lda"") 

- Quadratic Discriminant Analysis ("qda")

- Regularized Discriminant Analysis ("rda") 

- Random Forest ("rf") 

- Neural Network ("nnet")

- Neural Networks with Feature Extraction ("pcaNNet")

- Bagged AdaBoost ("AdaBag")

- Support Vector Machines with Radial Basis Function Kernel ("svmRadial")

- Penalized Multinomial Regression ("multinom") 

- k-Nearest Neighbors ("kknn")

There are numerous possibilities for tuning many of these models, but I didn't do any tuning.

## Files

I have placed all the code to run this project in a file called [pml_analysis.R](pml_analysis.R), which produces the output in [pml_output.txt](pml_output.txt) (as well as cut and pasted console messages in [pml_messages.txt](pml_messages.txt)).  It uses the input data in [pml-training.csv](pml-training.csv) and [pml-testing.csv](pml-testing.csv) and produces an output data file (not human readable) [confusion.dat](confusion.dat), which I will use in presenting my results.  The full analysis took about 3 hours to run on my 2013-vintage MacBook Pro (with a QuadCore i7 chip).

## Results

On the initial validation data, some of the models performed well, and some didn't.  Here's what the kappa statistics look like for the 10 models, in decreasing order.

```{r}
load("confusion.dat")
kappas <- sapply(confuse, function(co) co[[3]]["Kappa"])
names(kappas) <- names(confuse)
print(sort(kappas,decreasing=TRUE))
```

The two most successful models, Random Forest and k-Nearest Neighbors, performed almost unbelievably well, and the Support Vector Machine model also performed very well.  The Quadratic Discriminant Analysis model also performed well.  (It appears from the more detailed results, available in [pml_output.txt](pml_output.txt), that the Regularized Discriminant Analysis has selected an exactly equivalent model.)

I combined the 3 best-performing models in a "weighted majority vote" setup which chooses any label with 2 votes and allows the Random Forest model to break ties (i.e., when all three disagree).  On the initial validation set, the performance of the combined model was as follows:

```{r}
print(confuse_voted_1stpass)
```

This is not clearly better than the performance of the Random Forest model alone:
```{r}
print(confuse["rf"])
```

However, it's not really fair to compare an alternative procedure with the best performing single model on the same data set where it has just shown itself to be the best performing single model.  Also, the combined model is (slightly) better at distinguishing Class A from the other classes.  They got the same number of true positives but RF has one more false positive.  This is important because A vs. the other classes represents "the right way" vs. "the wrong way" of performing the exercise.  Presumably we care more about "right way" vs. "wrong way" than about "Which wrong way?"  So I gave the combined model one more chance on the preliminary test set.

Here is the preliminary test set performance of the combined model (using fits that include the validation data):

```{r}
print(confuse_voted_2ndpass)
```

Compare the performance of the Random Forest model alone:

```{r}
print(conf["rf"])
```

Both perform excellently, but this seems to be an unambiguous win for the Random Forest model alone.  No need to waste time fitting the other models if they don't help.  (One might consider investigating whether a 4-model ensemble, with much more weight given to the Random Forest model, might perform better still, but I didn't do that.)

With 99.6% accuracy, the Random Forest model produced classification results that could be characterized as nearly perfect.  Allowing for the remaining degree of freedom on the preliminary test set (i.e. the choice between the Random Forest model and the combined model), I'd expect about 99% accuracy (mirroring the combined model's performance here) on any subsequent test data.
